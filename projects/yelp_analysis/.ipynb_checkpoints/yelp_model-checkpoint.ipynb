{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    " \n",
    "import json\n",
    " \n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:39.482764\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "t = datetime.now()\n",
    "with open(\"data/dataset/review.json\") as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "    \n",
    "reviews = [json.loads(review) for review in reviews]\n",
    "print(datetime.now() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 100000, 1: 100000})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a balanced sample of pos and neg reviews\n",
    "texts = [review['text'] for review in reviews]\n",
    "\n",
    "# Make binary classifier out of date pos and neg\n",
    "binstars = [0 if review['stars'] <= 3 else 1 for review in reviews]\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "limit = 100000\n",
    "neg_pos_counts = [0, 0]\n",
    "for i in range(len(texts)):\n",
    "    polarity = binstars[i]\n",
    "    if neg_pos_counts[polarity] < limit:\n",
    "        balanced_texts.append(texts[i])\n",
    "        balanced_labels.append(polarity)\n",
    "        neg_pos_counts[polarity] += 1\n",
    "    \n",
    "Counter(balanced_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...,  688  249   84]\n",
      " [   0    0    0 ...,  441    2  122]\n",
      " [   0    0    0 ...,   51    1 1081]\n",
      " ..., \n",
      " [   0    0    0 ...,    2    1 4743]\n",
      " [   0    0    0 ...,   29   54  107]\n",
      " [   0    0    0 ...,    2 1513  261]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the texts into vectors!\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=300)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/3\n",
      "100000/100000 [==============================] - 383s 4ms/step - loss: 0.2862 - acc: 0.8798 - val_loss: 0.3867 - val_acc: 0.8371\n",
      "Epoch 2/3\n",
      "100000/100000 [==============================] - 376s 4ms/step - loss: 0.2015 - acc: 0.9210 - val_loss: 0.2612 - val_acc: 0.8893\n",
      "Epoch 3/3\n",
      "100000/100000 [==============================] - 380s 4ms/step - loss: 0.1565 - acc: 0.9401 - val_loss: 0.3497 - val_acc: 0.8706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcae06305f8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model. We need to go deeper!\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length = 300))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, 5, activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 4))\n",
    "\n",
    "model.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3) # Train rocky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "   pickle.dump(tokenizer, f)\n",
    "model.save(\"yelp_sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9240576 ]\n",
      " [ 0.79750961]\n",
      " [ 0.13258116]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    " \n",
    "# load the tokenizer and the model\n",
    "with open(\"keras_tokenizer.pickle\", \"rb\") as f:\n",
    "   tokenizer = pickle.load(f)\n",
    " \n",
    "model = load_model(\"yelp_sentiment_model.hdf5\")\n",
    " \n",
    "# replace with the data you want to classify\n",
    "newtexts = [\"Your new data\", \"More new data\", \"Everything sucks\"]\n",
    " \n",
    "# note that we shouldn't call \"fit\" on the tokenizer again\n",
    "sequences = tokenizer.texts_to_sequences(newtexts)\n",
    "data = pad_sequences(sequences, maxlen=300)\n",
    " \n",
    "# get predictions for each of your new texts\n",
    "predictions = model.predict(data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
